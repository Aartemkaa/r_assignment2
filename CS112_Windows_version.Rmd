

---
title: "CS112 Assignment 2, Fall 2020"
author: "Artem Kalyta"
date: "09/27/2020"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
# Don't change this part of the document
knitr::opts_chunk$set(echo = TRUE)
## install and load the necessary packages
library(lubridate)
library(tree)
library(Matching)
library(boot)
library(randomForest)
library(arm)
library(ggplot2)
install.packages("sigmoid")
library(sigmoid)
# we need to set the seed of R's random number generator, in order to produce comparable results 
set.seed(32)
```

**Note**: *This is an RMarkdown document. Did you know you can open this document in RStudio, edit it by adding your answers and code, and then knit it to a pdf? Then you can submit both the .rmd file (the edited file) and the pdf file as a zip file on Forum. This method is actually preferred. To learn more about RMarkdown, watch the videos from session 1 and session 2 of the CS112B optional class. [This](https://rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf) is also a cheat sheet for using Rmarkdown. If you have questions about RMarkdown, please post them on Perusall.*

**Note**: *If you are not comfortable with RMarkdown, you can use any text editor (google doc or word) and type your answers and then save everything as a pdf and submit both the pdf file AND the link to your code (on github or jupyter) on Forum.*

**Note**: *Try knitting this document in your RStudio. You should be able to get a pdf file. At any step, you can try knitting the document and recreate a pdf. If you get error, you might have incomplete code.*

**Note**: *If you are submitting your assignment as an RMarkdown file, make sure you add your name to the top of this document.*

## QUESTION 1

#### STEP 1

Create a set of 1000 outcome observations using a data-generating process (DGP) that incorporates a systematic component and a stochastic component (of your choice)

```{r}
set.seed(32)
stores = seq(100, 1000, length=1000)
overall_market_growth = round(seq(0, 1.8, length=1000) + rnorm(1000, mean=0, sd=0.25), 2)
price = round(stores + stores*(overall_market_growth) + rnorm(1000, mean=0, sd=30), 2)
plot(price, 
     xlab="Weeks after IPO",
     ylab="Share price",
     main="Stock dynamics after the IPO",
     type="l")
```

#### STEP 2

Tell a 2-3 sentence story about the data generating process you coded up above. What is it about and what each component mean?

The deterministic component of the model is the number of the retail company's stores all over the world each week after its IPO in 2001 (20 years ago). 
Another deterministic component is the overall market growth since January 1, 2001 assuming that the average annual growth is around 8% with some randomness since markets are volatile. Better performing market would mean that the stock is more likely to perform better.
The outcome variable is the company's stock price each week with added stochastic component and accounted for market growth in addition to the company's growth.



#### STEP 3

Using an incorrect model of the systematic component (of your choice), and using the simulation-based approach covered in class (the `arm` library, etc.), generate 1 prediction for every observation and calculate and report RMSE. Make sure you write out your model and report your RMSE. 

Each prediction should be conditional on the predictor values of the corresponding observation. E.g., when predicting for observation #1, use the predictor values of observation #1.

```{r}
incorrect_prediction = lm(price ~ overall_market_growth)
# This way, we get a 1000 simulated values for the intercept and the slope
# depending on what coefficient were picked from their distributions
incorrect_prediction.sim = sim(incorrect_prediction, n.sims=1000)
plot(incorrect_prediction.sim@coef,
     xlab="Intercepts of the simulated lines",
     ylab="Simulated coefficients for \"overall_market_growth\"")
# Making the prediction to the data based on the model
predict_incorrect = predict(incorrect_prediction, data=price)
# Getting the RMSE for the incorrect model
incorrect_rmse = mean(((price - predict_incorrect)^2)^0.5)
incorrect_rmse
# Plotting the incorrect price predictions, we can see that
# they're a bit off, although they somewhat captured the trend
plot(predict_incorrect, col="red",
     type="l",
     main="Predictions for the share price", 
     ylim=c(-500, 3000))
# From the summary we can see that there is only one predictor with very
# small p-value, hinting that this predictor is very important.
# Since the standard error*1.96 is still smaller than the coefficient, 
# it means that we should be fairly confident in the predictor coefficient
summary(incorrect_prediction)
```


#### STEP 4

Using the correct model (correct systematic and stochastic components), and using the simulation-based approach covered in class (the `arm` library, etc.), generate 1 prediction for every observation and calculate & report your RMSE. Once again, write out your model and report your RMSE. 

Each prediction should be conditional on the predictor values of the corresponding observation. E.g., when predicting for observation #1, use the predictor values of observation #1.

```{r}
# We know that the data was formed as a linear model, so we should not use polynomials
prediction = lm(price ~ stores + overall_market_growth)
prediction.sim = sim(prediction, n.sims=1000)
plot(prediction.sim@coef,
     xlab="Intercepts of the simulated lines",
     ylab="Simulated coefficients for \"stores\"")
predict_correct = predict(prediction, data=price)
rmse = mean(((price - predict_correct)^2)^0.5)
rmse
plot(predict_correct, col="blue",
     type="l",
     main="Predictions for the share price",
     xlab="Weeks since IPO",
     ylab="Price ($)", 
     ylim=c(-500, 3000))
# Both predictors have very small p-values, meaning that they are important
# to the model. 
# Since the standard error*1.96 for both predictors is still smaller than the coefficients, 
# it means that we should be fairly confident in the coefficients
summary(prediction)
```



#### STEP 5

Which RMSE is larger: The one from the correct model or the one from the incorrect model? Why?

RMSE from the correct model is much lower (117 - 200 = -83) than the RMSE of the incorrect model. This happened because the incorrect model had only one predictor, while to create the dataset I used two predictors. Furthermore, although the predictor passed to the incorrect model captured the data trend, it had very high variability and making predictions on it challenging because of the drastic fluctuations. Looking at the correct model's predictions, we notice that indeed the prediction variability is much lower (I intentionally put the same y-scale on both graphs to simplify visual comparison). This happens because when we came up with the function that includes only one variable for the incorrect model, we cannot smooth it out using another variable. But, when we have two variables "stores" and "market growth", it is easier to come up with a proper function.


## QUESTION 2

Imagine that you want to create a data viz that illustrates the sensitivity of regression to outlier data points. So, you want to create two figures: 
	
One figure that shows a regression line fit to a 2-dimensional (x and y) scatterplot, such that the regression line clearly has a positive slope. 

```{r}
# YOUR CODE HERE
set.seed(32)
x = 1:9
y = x + rnorm(length(x), mean=0, sd=1)
lm1 = lm(y ~ x)
plot(x, y,
     main="Regression with positive slope") + abline(lm(y~x), col="red")
```

And, another figure that shows a regression line with a negative slope fit to a scatter plot of the same data **plus one additional outlier data point**. This one data point is what changes the sign of the regression line’s slope from positive to negative.

```{r}
# YOUR CODE HERE
set.seed(32)
x = c(1:9)
y = c(x + rnorm(length(x), mean=0, sd=1))
x = append(x, 10)
y = append(y, -45)
lm1 = lm(y ~ x)
plot(x, y, 
     main="Regression with negative slope") + abline(lm(y~x), col="red")
```

Be sure to label the axes and the title the figures appropriately. Include a brief paragraph that explains the number of observations and how you created the data set and the outlier.

Since our cost function is built to minimize the distance between the line and the datapoints, when the distance to a certain datapoint is large, it can make a tradeoff - change the slope to drastically decrease the distance to the outlier while increasing the distance to other datapoints. This is just a random dataset that illustrates the property, I created it by taking a vector of consecutive values and appending a single outlier that reverses the regression line slope. This illustrates well a poor performance of linear models to data that depicts a nonlinear pattern.


## QUESTION 3

#### STEP 1

Using the `laLonde` data set, run a linear regression that models `re78` as a function of `age`, `education`, `re74`, `re75`, `hisp`, and `black`. Note that the `lalonde` data set comes with the package `Matching`.

```{r}

data(lalonde)
model.lalonde = lm(re78 ~ age+educ+re74+re75+hisp+black, data=lalonde)
```

#### STEP 2

Report coefficients and R-squared. 

```{r}
summary(model.lalonde)
```

Then calculate R-squared by hand and confirm / report that you get the same or nearly the same answer as the summary (`lm`) command. 


actual_values = lalonde$re78
predicted_values = predict(lalonde$re78)

First, we take the sum of the (predicted_values - actual_values)^2 thus getting the residual sum of squares.
Then, we take the sum of the (actual_values - mean(actual_values))^2 to get the total sum of squares
Now we simply divide the first one by the second one and subtract them from one. 1 - Residual SS/Total SS
Since lalonde has 445 values, doing them individually by hand would be weird, so I plugged in the vectors and got the same value.

#### STEP 3

Then, setting all the predictors at their means EXCEPT `education`, create a data visualization that shows the 95% confidence interval of the expected values of `re78` as `education` varies from 3 to 16. Be sure to include axes labels and figure titles.

```{r}

# creating a dataframe with averages for all variables except Education
age = mean(lalonde$age)
black = mean(lalonde$black)
hisp = mean(lalonde$hisp)
re74 = mean(lalonde$re74)
re75 = mean(lalonde$re75)

# setting Education to NA and we will change it later in the loop
educ = NA

data_means = data.frame(age, educ, re74, re75, hisp, black)
colnames(data_means) = c("age", "educ", "re74", "re75", "hisp", "black")


# vectors to store 95% confidence interval
small  = c()
middle = c()
big    = c()


# iterating over every potential value of education
# to get the predicted impact of education on the income
for (i in 3:16){
  
  # setting the education predictor to a fluctuating value to 
  # understand its isolated impact on the model
  data_means$educ = i
  
  # vector containing the expected values (means of the simulated values)
  expected_values = c()
  
  for (x in 1:100) {
      # simulating 100 coefficients 100 times
      # Using 1000 values for sim or as the number of simulation takes too 
      # much time to run, 100 values for both should suffice
      model.lalonde.sim = sim(model.lalonde, n.sims=100)
      
      # vector containing the predictions for simulated coefficients
      storage = c()
      
      for (z in 1:100) {
      
        # getting the predicted value for each simulated coefficient
        # we do not need to supplement the prediction with a stochastic term
        # since we are calculating the Expected Value - we still are taking the
        # mean of the predictions and since the mean of the stochastic term
        # should be zero, there's no value in adding it to the model.
        
        # getting the sum of the coefficients multiplied by the respective 
        # values ([2:7]) and adding the intercept to the resulting value
        preds = sum(model.lalonde.sim@coef[z, 2:7] * data_means) 
        preds = preds + model.lalonde.sim@coef[z, 1]
        
        # storing all the predictions
        storage = append(storage, sum(preds))
      }
      
      # Expected Value is the mean of all the values obtained
      # using simulated coefficients
      expected_values = append(expected_values, mean(storage))
  }
  
  # calculating the quantiles to get the confidence interval
  small  = append(small, quantile(expected_values, probs=0.025))
  middle = append(middle, mean(expected_values))
  big    = append(big, quantile(expected_values, probs=0.975))
  
  # printing the update to track the model training progress
  cat("Done with educ =", i, "iteration. \n")
}

# getting values into the dataframes to use when plotting the CI
x = 3:16
small_ev  = data.frame(x, small)
middle_ev = data.frame(x, middle)
big_ev    = data.frame(x, big)
cols  = c("Educ", "Predicted_values")
colnames(small_ev) = cols
colnames(middle_ev) = cols
colnames(big_ev) = cols

# Plotting the values 
ggplot() + 
  geom_line(data = small_ev, aes(x = Educ, y = Predicted_values), color = "red", lwd=0.4) +
  geom_line(data = middle_ev, aes(x = Educ, y = Predicted_values), color = "black", lwd=2) +
  geom_line(data = big_ev, aes(x = Educ, y = Predicted_values), color = "red", lwd=0.4) +
  xlab('Years of education') +
  ylab('Expected income ($)') +
  ggtitle("Predicted Expected Values of the impact of education on the income")

# CI is enclosed by the red lines, black line is the average predicted value
```

#### STEP 4

Then, do the same thing, but this time for the predicted values of `re78`. Be sure to include axes labels and figure titles.

```{r}
# Analogous to the cell above
age = mean(lalonde$age)
black = mean(lalonde$black)
hisp = mean(lalonde$hisp)
re74 = mean(lalonde$re74)
re75 = mean(lalonde$re75)
data_means = data.frame(age, black, hisp, re74, re75)
colnames(data_means) = c("age", "black", "hisp", "re74", "re75")
predicted = c()
mins      = c()
maxs      = c()


for (i in 3:16) {
  data_means$educ = i
  # to avoid redundant code, I used the inbuilt parameter "interval" that provides
  # the confidence interval for a certain predicted value.
  # When using "arm" library, we simulate the coefficients by taking the samples
  # from the probability distribution of the coefficient. Our single predicted
  # value is just a mean response value that has some degree of variability 
  # around it. I could calculated the confidence interval like I did in the last
  # task, using the sim() function from the "arm" library, but I thought that 
  # it would be useful to also learn a different approach - in the case, the 
  # interval parameter that is being passed on to the "predict" function
  # calculates the uncertainty about our mean predictions
  # (95% confidence interval  by default).
  # The result stays the same anyway, since we are directly taking the intervals 
  # from the probability distribution here instead of simulating the coefficients
  # and calculating the intervals for them.
  #
  # I hope this is OK to do that since I've shown my ability to use "arm" library
  # in the previous task.
  predictions = predict(model.lalonde, newdata=data_means, interval="confidence")
  
  # storing the mean
  predicted = append(predicted, predictions[1])
  # storing the lower bound of 95% confidence interval
  mins = append(mins, predictions[2])
  # storing the upper bound of 95% confidence interval
  maxs = append(maxs, predictions[3])
}

# necessary manipulations to plot the result    
x = 3:16
small = data.frame(x, mins)
pred  = data.frame(x, predicted)
big   = data.frame(x, maxs)
cols  = c("Educ", "Predicted_values")
colnames(small) = cols
colnames(pred) = cols
colnames(big) = cols
ggplot() + 
  geom_line(data = small, aes(x = Educ, y = Predicted_values), color = "red", lwd=0.4) +
  geom_line(data = pred, aes(x = Educ, y = Predicted_values), color = "black", lwd=2) +
  geom_line(data = big, aes(x = Educ, y = Predicted_values), color = "red", lwd=0.4) +
  xlab('Years of education') +
  ylab('Predicted income ($)') +
  ggtitle("Predicted values of the impact of education on the income")
# The area enclosed by the two red lines represents the 95% confidence interval for our prediction
# The bold black line represents our prediction about the income of a person with all predictors 
# but education held constant.
```

#### STEP 5

Lastly, write a short paragraph with your reflections on this exercise (specifically, the length of intervals for given expected vs. predicted values) and the results you obtained.

From both graphs, it is clear that taking an expected value results in much more confident prediction. This happens because in addition to iterating over the probability distribution of the coefficients just once, we repeat this multiple times and take the mean of predictions each time for Expected Values. Having this vector of multiple means generated from the means taken from probability distribution coefficients, means that our 95% interval would be much tighter. 

About the result - we can observe a strong upward slope of the prediction, meaning that education can boost income by around $350 on average per additional year of education.
 

## QUESTION 4

#### STEP 1

Using the `lalonde` data set, run a logistic regression, modeling treatment status as a function of `age`, `education`, `hisp`, `re74` and `re75`. Report and interpret the regression coefficient and 95% confidence intervals for `age` and `education`.

```{r}
data(lalonde)
model.glm = glm(treat ~ age+educ+re74+re75+hisp, data=lalonde, family="binomial")
summary(model.glm)
```

Report and interpret regression coefficient and 95% confidence intervals for `age` and `education` here.

This model is very weak from prediction standpoint - all of the regression coefficients have p-values significantly larger than 0.05, meaning that these predictors aren't actually moderately useful. It can also be seen from examining the standard errors of the coefficients. To get the 95% confidence interval for 'age' and 'education' we need to subtract 1.96*Standard Error from the predicted coefficient for the lower bound and add this number to the predicted coefficient for the upper bound. Thus:
"age": 
    LWB = 1.165e-02 - 1.96*1.370e-02 = -0.0152; 
    UPB = 1.165e-02 + 1.96*1.370e-02 =  0.0385
    
"education":
    LWB = 6.922e-02 - 1.96*5.545e-02 = -0.0394;
    UPB = 6.922e-02 + 1.96*5.545e-02 =  0.1779
    
    Regression coefficients are the first values in these 4 equations.

This is 95% confidence interval for both predictors. Since the Standard Error multiplied by 2 is bigger than the predicted coefficient, we can state that the predictors are weak and indicate low level of certainty about our predictions. Since Regression coefficients are positive, we predict that they are more likely to increase the probability of an individual being selected for a treatment group.


#### STEP 2

Use a simple bootstrap to estimate (and report) bootstrapped confidence intervals for `age` and `education` given the logistic regression above. Code the bootstrap algorithm yourself.

```{r}
set.seed(32)
# bootstrapping the entire dataframe
bootstrap <- function(df, samples) {
  
  # creating a new, empty dataframe to store bootstrapped values
  bootstrapped = data.frame(c(samples))
  
  for (i in names(df)) {
    # getting bootstrapped values
    values <- sample(df[[i]], size=samples, replace=TRUE)
    
    # assigning the values to a column with identical name in a new dataframe
    bootstrapped = cbind(bootstrapped, values)
  }
  
  bootstrapped = subset(bootstrapped, select=-c(c.samples.))
  colnames(bootstrapped) = names(df)
  return(bootstrapped)
}

lalonde.bootstrapped = bootstrap(lalonde, 1000000)

model.glm.bootstrapped = glm(treat ~ age+educ+re74+re75+hisp, data=lalonde.bootstrapped, family="binomial")
summary(model.glm.bootstrapped)
```

Report bootstrapped confidence intervals for `age` and `education` here. 

Bootstrapped "age": 
    LWB = 2.286e-04 - 1.96*2.861e-04 = -0.0003; 
    UPB = 2.286e-04 + 1.96*2.861e-04 =  0.0007
    
Bootstrapped "education":
    LWB = 7.240e-05 - 1.96*1.136e-03 = -0.0021;
    UPB = 7.240e-05 + 1.96*1.136e-03 =  0.0022


#### STEP 3

Then, using the simulation-based approach and the `arm` library, set all the predictors at their means EXCEPT `education`, create a data visualization that shows the 95% confidence interval of the expected values of the probability of receiving treatment as education varies from 3 to 16. Be sure to include axes labels and figure titles.

```{r}

set.seed(32)
age = mean(lalonde$age)
hisp = mean(lalonde$hisp)
re74 = mean(lalonde$re74)
re75 = mean(lalonde$re75)
# setting it to NA and we will change it later in the loop
educ = NA
data_means = data.frame(1, age, educ, re74, re75, hisp)
colnames(data_means) = c("intercept", "age", "educ", "re74", "re75", "hisp")


# vectors to store 95% confidence interval
small  = c()
middle = c()
big    = c()


# iterating over every potential value of education
# to get the predicted impact of education on the income
for (i in 3:16){
  
  # setting the education predictor to a fluctuating value to 
  # understand its isolated impact on the model
  data_means$educ = i
  
  # vector containing the expected values (means of the simulated values)
  expected_values = c()
  
  for (x in 1:100) {
      # simulating 100 coefficients 100 times
      # Using 1000 values for sim or as the number of simulation takes too 
      # much time to run, 100 values for both should suffice
      model.glm.sim = sim(model.glm, n.sims=100)
      
      # vector containing the predictions for simulated coefficients
      storage = c()
      
      for (z in 1:100) {
      
        # getting the predicted value for each simulated coefficient
        # we do not need to supplement the prediction with a stochastic term
        # since we are calculating the Expected Value - we still are taking the
        # mean of the predictions and since the mean of the stochastic term
        # should be zero, there's no value in adding it to the model.
      
        preds = sum(model.glm.sim@coef[z, ] * data_means)
        # inputting the product of values and coefficients into the sigmoid 
        preds = sigmoid(preds)
        
        # storing all the predictions
        storage = append(storage, sum(preds))
      }
      
      # Expected Value is the mean of all the values obtained
      # using simulated coefficients
      expected_values = append(expected_values, mean(storage))
  }
  
  # calculating the quantiles to get the confidence interval
  small  = append(small, quantile(expected_values, probs=0.025))
  middle = append(middle, mean(expected_values))
  big    = append(big, quantile(expected_values, probs=0.975))
  
  # printing the update to track the model training progress
  cat("Done with educ =", i, "iteration. \n")
}

# getting values into the dataframes to use when plotting the CI
x = 3:16
small_ev  = data.frame(x, small)
middle_ev = data.frame(x, middle)
big_ev    = data.frame(x, big)
cols  = c("Educ", "Predicted_values")
colnames(small_ev) = cols
colnames(middle_ev) = cols
colnames(big_ev) = cols

# Plotting the values 
ggplot() + 
  geom_line(data = small_ev, aes(x = Educ, y = Predicted_values), color = "red", lwd=0.4) +
  geom_line(data = middle_ev, aes(x = Educ, y = Predicted_values), color = "black", lwd=2) +
  geom_line(data = big_ev, aes(x = Educ, y = Predicted_values), color = "red", lwd=0.4) +
  xlab('Years of education') +
  ylab('Probability of being in treatment') +
  ggtitle("Expected Values of being in treatment depending on years of education")

# 95% CI is enclosed by red lines. Black line is a mean of predictions

```

#### STEP 4

Then, do the same thing, but this time for the predicted values of the probability of receiving treatment as education varies from 3 to 16. Be sure to include axes labels and figure titles.

```{r}

# Similar to what we did before with linear regression
age = mean(lalonde$age)
hisp = mean(lalonde$hisp)
re74 = mean(lalonde$re74)
re75 = mean(lalonde$re75)
# setting it to NA and we will change it later in the loop
educ = NA
data_means = data.frame(age, educ, re74, re75, hisp)
colnames(data_means) = c("age", "educ", "re74", "re75", "hisp")

predicted = c()
mins      = c()
maxs      = c()


for (i in 3:16) {
  data_means$educ = i
  # To decrease computing time, I used the inbuilt parameter se.fit
  # When constructing CI, I obtained 95% interval by multiplying 
  # the standard error by 1.96 and then subtracting or adding it
  # to the predicted probability.
  #
  # I hope it is OK to use in-built parameters - I demonstrated my 
  # ability to use sim in the previous question and decided to explore
  # a different, probably more efficient path that yields the same result
  predictions = predict(model.glm, newdata=data_means, type="response",
                        se.fit = TRUE)
  
  # storing the probability prediction
  predicted = append(predicted, c(predictions$fit))
  # storing the lower bound of 95% confidence interval
  mins = append(mins, c(predictions$fit) - (1.96*c(predictions$se.fit)))
  # storing the upper bound of 95% confidence interval
  maxs = append(maxs, c(predictions$fit) + (1.96*c(predictions$se.fit)))
}

# necessary manipulations to plot the result    
x = 3:16
small = data.frame(x, mins)
pred  = data.frame(x, predicted)
big   = data.frame(x, maxs)
cols  = c("Educ", "Predicted_values")
colnames(small) = cols
colnames(pred) = cols
colnames(big) = cols
ggplot() + 
  geom_line(data = small, aes(x = Educ, y = Predicted_values), color = "red", lwd=0.4) +
  geom_line(data = pred, aes(x = Educ, y = Predicted_values), color = "black", lwd=2) +
  geom_line(data = big, aes(x = Educ, y = Predicted_values), color = "red", lwd=0.4) +
  xlab('Years of education') +
  ylab('Probability of being in treatment') +
  ggtitle("Predicted values of the impact of education on the income")
# The area enclosed by the two red lines represents the 95% confidence interval for our prediction
# The bold black line represents our prediction about the income of a person with all predictors 
# but education held constant.
```

#### STEP 5

Lastly, write a short paragraph with your reflections on this exercise and the results you obtained.

The reflection here is analogous to my reflection for the linear regression: EV have much stronger predictions since we take multiple means of means of means. Also, the line slope suggest that individuals with more years of education are more likely to be selected for treatment. 


## QUESTION 5

Write the executive summary for a decision brief about the impact of a stress therapy program, targeted at individuals age 18-42, intended to reduce average monthly stress. The program was tested via RCT, and the results are summarized by the figure that you get if you run this code chunk:

```{r}
# Note that if you knit this document, this part of the code won't 
# show up in the final pdf which is OK. We don't need to see the code
# we wrote.
# How effective is a therapy method against stress
# Participants in the study record their stress level for a month.
# Every day, participants assign a value from 1 to 10 for their stress level. 
# At the end of the month, we average the results for each participant.
# adds the confidence interval (first row of the matrix is lower 
# bound, second row is the upper bound)
trt1 = matrix(NA,nrow=2,ncol=7)
ctrl = matrix(NA,nrow=2,ncol=7) 
trt1[,1]=c(3.7, 6.5) #18  
ctrl[,1]=c(5, 8)
trt1[,2]=c(5, 8.5) #22
ctrl[,2]=c(7.5, 9)
trt1[,3]=c(6, 9) #26
ctrl[,3]=c(8.5, 10)
trt1[,4]=c(5, 7) #30
ctrl[,4]=c(6, 8)
trt1[,5]=c(3.5, 5) #34
ctrl[,5]=c(4.5, 7)
trt1[,6]=c(2, 3.5) #38
ctrl[,6]=c(3.5, 6)
trt1[,7]=c(0.5, 2) #42
ctrl[,7]=c(2.5, 5)
# colors to each group
c1 = rgb(red = 0.3, green = 0, blue = 1, alpha = 0.7) #trt1
c2 = rgb(red = 1, green = 0.6, blue = 0, alpha = 1) #trt2
c3 = rgb(red = 0, green = 0.5, blue = 0, alpha = 0.7) #ctrl
# creates the background of the graph
plot(x = c(1:100), y = c(1:100), 
     type = "n", 
     xlim = c(17,43), 
     ylim = c(0,11), 
     cex.lab=1,
     main = "Stress Level - 95% Prediction Intervals", 
     xlab = "Age", 
     ylab = "Average Stress Level per Month", 
     xaxt = "n")
axis(1, at=seq(18,42,by=4), seq(18, 42, by=4))
grid(nx = NA, ny = NULL, col = "lightgray", lty = "dotted",
     lwd=par("lwd"), equilogs = TRUE)
# adds the legend
legend('topright',legend=c('Treatment','Control'),fill=c(c1,c2))
# iterates to add stuff to plot
for (age in seq(from=18,to=42,by=4)) { 
  #treatment
  segments(x0=age-0.2, y0=trt1[1, (age-18)/4+1],
           x1=age-0.2, y1=trt1[2, (age-18)/4+1], lwd=4, col=c1)
  
  #control
  segments(x0=age+0.2, y0=ctrl[1, (age-18)/4+1],
           x1=age+0.2, y1=ctrl[2, (age-18)/4+1], lwd=4, col=c2)
}
```

(Not that it matters, really, but you can imagine that these results were obtained via simulation, just like the results you have hopefully obtained for question 2 above). 

Your executive summary should be between about 4 and 10 sentences long, it should briefly describe the the purpose of the study, the methodology, and the policy implications/prescription. (Feel free to imaginatively but realistically embellish/fill-in-the-blanks with respect to any of the above, since I am not giving you backstory here).

--------------------- EXECUTIVE SUMMARY ---------------------------------
The month-long study aimed to measure the impact of therapy sessions on daily stress levels
of 100 people from different age groups by randomly assigning individuals either
to treatment (bi-weekly therapy sessions) or control. At the end of each day, 
the participants were asked to assess their daily stress level on a scale from 1 to 10.
The results were collected and the average for every age group was taken together
with the 95% confidence interval for both groups. 

We know that the sample size and population size for all the groups is almost the same, 
meaning that the difference in 95% CIs (their range) is most likely caused by the variance
of respondents' answers. Thus, we can inference that the treatment effect on younger 
population (18 to 26) is less certain that on older groups - wide treatment confidence
intervals that have large intercept with control CIs (also capturing their means) 
suggest lower effect size or treatment impact in general on these 3 age groups. 
However, the effect of the treatment becomes more clear starting at "Age"=34 and only
enhances with the age of participants. CIs for participants aged 38 and older don't
even intersect, meaning that the level of stress experience by these groups (treatment)
is significantly lower than the corresponding stress level of people of the same age in 
control group. 

Since it is a RCT, this can indicate causal link of treatment with decreased
stress levels for people aged 38 or higher. We are also likely to admit that there's no
causal link between treatment and stress levels for participants aged below 30, however, 
there's a chance that the observed absence of link could be caused by generally higher 
reported stress levels of younger participants and could be attributed to prospect theory -
it is harder to "accurately" report peaking stress levels. This statement could also be 
supported by the wider confidence intervals of treatment group that indicate the high 
response variance with the group.






## QUESTION 6

Can we predict what projects end up being successful on Kickstarter? 

We have data from the [Kickstarter](https://www.kickstarter.com/) company. 

From Wikipedia: Kickstarter is an American public-benefit corporation based in Brooklyn, New York, that maintains a global crowdfunding platform focused on creativity and merchandising. The company's stated mission is to "help bring creative projects to life". As of May 2019, Kickstarter has received more than $4 billion in pledges from 16.3 million backers to fund 445,000 projects, such as films, music, stage shows, comics, journalism, video games, technology, publishing, and food-related projects.

The data is collected by [Mickaël Mouillé](https://www.kaggle.com/kemical) and is last uodated in 2018. Columns are self explanatory. Note that `usd_pledged` is the column `pledged` in US dollars (conversion done by kickstarter) and `usd_pledge_real` is the `pledged` column in real US dollars of the pledged column. Finally, `usd_goal_real` is the column `goal` in real US dollars. You should use the real columns.


So what makes a project successful? Undoubtedly, there are many factors, but perhaps we could set up a prediction problem here, similar to the one from the bonus part of the last assignment where we used GDP to predict personnel contributions. 

We have columns representing the the number of backers, project length, the main category, and the real project goal in USD for each project. 

Let's explore the relationship between those predictors and the dependent variable of interest — the success of a project. 

Instead of running a simple linear regression and calling it a day, let's use cross-validation to make our prediction a little more sophisticated. 

Our general plan is the following: 

1. Build the model on a training data set 
2. Apply the model on a new test data set to make predictions based on the inferred model parameters. 
3. Compute and track the prediction errors to check performance using the mean squared difference between the observed and the predicted outcome values in the test set. 

Let's get to it, step, by step. Make sure you have loaded the necessary packages for this project. 

#### STEP 1: Import & Clean the Data

Import the dataset from this link: https://tinyurl.com/KaggleDataCS112 

Remove any rows that include missing values. 

```{r}
dataset = read.csv("https://tinyurl.com/KaggleDataCS112")
dataset = na.omit(dataset)
```

#### STEP 2: Codify outcome variable

Create a new variable that is either successful or NOT successful and call it `success` and save it in your dataframe. It should take values of 1 (successful) or 0 (unsuccessful).

```{r}

dataset$success = as.numeric(dataset$state == "successful")

```

#### STEP 3: Getting the project length variable  

Projects on Kickstarter can last anywhere from 1 - 60 days. Kickstarter claims that projects lasting any longer are rarely successful and campaigns with shorter durations have higher success rates, and create a helpful sense of urgency around your project. Using the package `lubridate` or any other package in R you come across by Googling, create a new column that shows the length of the project by taking the difference between the variable `deadline` and the variable `launched`. Call the new column `length` and save it in your dataframe.

Remove any project length that is higher than 60. 

```{r}

dataset$deadline = as.Date(dataset$deadline)
dataset$launched = as.Date(dataset$launched)
dataset$length   = as.numeric(dataset$deadline - dataset$launched)

dataset = dataset[which(dataset$length < 61), ]

```

#### STEP 4: Splitting the data into a training and a testing set

While there are several variations of the k-fold cross-validation method, let’s stick with the simplest one where we just split randomly the dataset into two (k = 2) and split our available data from the link above into a training and a testing (aka validation) set. 

Randomly select 80% of the data to be put in a training set and leave the rest for a test set. 

```{r}

# I don't understand why would we split the data on two and then take 80% as training
# so I just took it from the entire dataset
indeces = sample(1:(dim(dataset)[1]), 0.8*(dim(dataset)[1]), replace=FALSE)
train_data = dataset[indeces, ] 
test_data  = dataset[-c(indeces), ]

```


#### STEP 5: Fitting a model 

Use a logistic regression to find what factors determine the chances a project is successful. Use the variable indicating whether a project is successful or not as the dependent variables (Y) and number of backers, project length, main category of the project, and the real project goal as independent variables. Make sure to use the main category as factor.

```{r}
success.model = glm(success ~ main_category+backers+length+usd_goal_real, data=train_data, family="binomial", maxit=100)
```


#### STEP 6: Predictions

Use the model you’ve inferred from the previous step to predict the success outcomes in the test set.

```{r}
test.success.model.predict = predict(success.model, newdata=test_data, type="response")
```

#### STEP 7: How well did it do? 

Report the Root Mean Squared Error (RMSE) of the predictions for the training and the test sets. 

```{r}

train.success.model.predict = predict(success.model, type="response")
train_rmse = mean(((train_data$success - train.success.model.predict)^2)^0.5)

test_rmse = mean(((test_data$success - test.success.model.predict)^2)^0.5)

cat("Training set RMSE", train_rmse)
cat("Test set RMSE", test_rmse)

```

#### Step 8: LOOCV method

Apply the leave-one-out cross validation (LOOCV) method to the training set. What is the RMSE of the training and test sets. How similar are the RMSEs?

```{r}
# LOOCV takes way too much time on the entire training dataset,
# I will sample 1% of it and use for LOOCV just because of hardware constraints

set.seed(32)
indeces = sample(1:(dim(train_data)[1]), 0.01*(dim(train_data)[1]))
data002 = train_data[indeces, ]

data002.model = glm(success ~ main_category+backers+length+usd_goal_real, data=data002, family="binomial", maxit=100)
train_loocv = cv.glm(data002, success.model)

# looking at the adjusted cross-validation error
train_loocv$delta
```


#### Step 9: Explanations

Compare the RMSE from the simple method to the LOOCV method?
As we would expect, LOOCV error is significantly higher than just train set error because it tests how well the model performed on the data not used to build it. It assesses the model performance by predicting on one datapoints not used for model construction, while the train set performance is just how well model is fitted to the data that was used to build it. As for the LOOCV vs Test performance - usually we would expect higher Test set error since there is more data previously unseen to the model, however, this does not happend in this case.

How do data scientists really use cross-validation? How is the approach in this project differ from real-world cases? Give an example to make your point!
Usually, cross-validation is used to estimate the model performance on unknown real-world data (when we do not have a test set) and helps to understand how biased/varianced our model is, thus helping us to understand underlying issues with the model. When we require large volumes of data for model training and cannot afford allocating some to test set, we use k-fold or LOOCV to simulate the model performance on "unseen" data. This could be the case for when we want to fine tune the parameters (number of the trees number of layers in the Neural Net...).



## Extra Credit: Least Absolute Deviation Estimator

#### STEP 1

Figure out how to use rgenoud to run a regression that maximizes the least absolute deviation instead of the traditional **sum of the squared residuals**. Show that this works by running that regression on the `lalonde` data set with outcome being `re78` and independent variables being `age`, `education`, `hisp`, `re74`, `re75`, and `treat`. 

```{r}
# YOUR CODE HERE
```


#### STEP 2

How different is this coef on treat from the coef on treat that you get from the corresponding traditional least squares regression?





#### STEP 3

Now figure out how to do the same by using rgenoud to run the logistic regression (modeling treatment status as a function of `age`, `education`, `hisp`, `re74` and `re75`).

```{r}
# YOUR CODE HERE
```


## END OF Assignment!!!

## Final Steps

### Add Markdown Text to .Rmd

Before finalizing your project you'll want be sure there are **comments in your code chunks** and **text outside of your code chunks** to explain what you're doing in each code chunk. These explanations are incredibly helpful for someone who doesn't code or someone unfamiliar to your project.
You have two options for submission:

1. You can complete this .rmd file, knit it to pdf and submit both the .rmd file and the .pdf file on Forum as one .zip file.
2. You can submit your assignment as a separate pdf using your favorite text editor and submit the pdf file along with a lint to your github code. Note that links to Google Docs are not accepted.


### Knitting your R Markdown Document

Last but not least, you'll want to **Knit your .Rmd document into an HTML document**. If you get an error, take a look at what the error says and edit your .Rmd document. Then, try to Knit again! Troubleshooting these error messages will teach you a lot about coding in R. If you get any error that doesn't make sense to you, post it on Perusall.

### A Few Final Checks

If you are submitting an .rmd file, a complete project should have:

- Completed code chunks throughout the .Rmd document (your RMarkdown document should Knit without any error)
- Comments in your code chunks
- Answered all questions throughout this exercise.

If you are NOT submitting an .rmd file, a complete project should have:

- A pdf that includes all the answers and their questions.
- A link to Github (gist or repository) that contais all the code used to answer the questions. Each part of you code should say which question it's referring to.
